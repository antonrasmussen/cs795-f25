## ğŸ§­ Canon of Generative AI

### 1. **Auto-Encoding Variational Bayes (2013)**

*Kingma & Welling*

* Introduced **Variational Autoencoders (VAEs)**.
* Key innovation: the **reparameterization trick**, enabling efficient training of latent-variable generative models.
* Impact: foundational for probabilistic generative modeling, especially in scientific domains.
  ğŸ“„ [arXiv:1312.6114](https://arxiv.org/abs/1312.6114)

---

### 2. **Generative Adversarial Networks (GANs) (2014)**

*Goodfellow et al.*

* Proposed **adversarial training**: a generator vs. discriminator in a minimax game.
* GANs produced far sharper images than VAEs at the time.
* Sparked an entire subfield (DCGAN, StyleGAN, CycleGAN, etc.).
  ğŸ“„ [arXiv:1406.2661](https://arxiv.org/abs/1406.2661)

---

### 3. **Neural Autoregressive Distribution Estimation (NADE & PixelRNN/PixelCNN, 2014â€“2016)**

*Bengio et al., van den Oord et al.*

* Explored **autoregressive models** for density estimation and image generation.
* PixelCNN demonstrated pixel-by-pixel image generation, inspiring later diffusion approaches.
  ğŸ“„ [Pixel Recurrent Neural Networks (2016)](https://arxiv.org/abs/1601.06759)

---

### 4. **Attention Is All You Need (2017)**

*Vaswani et al.*

* Introduced the **Transformer** architecture, removing recurrence entirely.
* Core mechanism: **self-attention**, scalable and parallelizable.
* Foundation for GPT, BERT, and almost every modern LLM.
  ğŸ“„ [arXiv:1706.03762](https://arxiv.org/abs/1706.03762)

---

### 5. **Improving Language Understanding by Generative Pre-Training (GPT-1, 2018)**

*Radford et al. (OpenAI)*

* First demonstration of **pretraining a transformer on large text corpora** and fine-tuning for tasks.
* Proof that large, generative pretraining transfers across NLP tasks.
  ğŸ“„ [PDF](https://cdn.openai.com/research-covers/language-unsupervised/language_understanding_paper.pdf)

---

### 6. **StyleGAN (2019)**

*Karras et al. (NVIDIA)*

* Major leap in **image synthesis**, enabling high-quality, high-resolution, and controllable outputs.
* Introduced the â€œstyle-basedâ€ generator architecture.
* Famous for producing photorealistic but synthetic human faces.
  ğŸ“„ [arXiv:1812.04948](https://arxiv.org/abs/1812.04948)

---

### 7. **BERT (2019)** *(not purely generative but pivotal)*

*Devlin et al.*

* Bidirectional transformer, popularized **masked language modeling**.
* Though discriminative, BERT inspired hybrid approaches and clarified pretraining strategies.
  ğŸ“„ [arXiv:1810.04805](https://arxiv.org/abs/1810.04805)

---

### 8. **Diffusion Models Beat GANs (2021)**

*Dhariwal & Nichol (OpenAI)*

* Revived **diffusion probabilistic models** as superior to GANs for image generation.
* Basis for **DALLÂ·E 2, Imagen, Stable Diffusion**.
  ğŸ“„ [arXiv:2105.05233](https://arxiv.org/abs/2105.05233)

---

## ğŸ“ Summary Timeline

* **2013** â€“ VAEs (probabilistic latent modeling)
* **2014** â€“ GANs (adversarial synthesis)
* **2016** â€“ PixelCNN (autoregressive image models)
* **2017** â€“ Transformer (attention revolution)
* **2018** â€“ GPT-1 (generative pretraining for language)
* **2019** â€“ StyleGAN (photorealistic image synthesis)
* **2019** â€“ BERT (contextual pretraining, hybrid influence)
* **2021** â€“ Diffusion (state-of-the-art image generation)

---

ğŸ‘‰ Together, these eight papers form the **core syllabus** of generative AI. Reading them in order gives you the intellectual trajectory from *probabilistic inference â†’ adversarial training â†’ autoregressive modeling â†’ attention â†’ large-scale pretraining â†’ controllable synthesis â†’ diffusion dominance*.


---

## ğŸŒ± Bonus Influential Papers in Generative AI

### ğŸ”„ Image-to-Image & Domain Transfer

* **CycleGAN (2017)** â€“ *Zhu et al.*

  * Enabled **unpaired image-to-image translation** (e.g., horses â†” zebras).
  * Landmark for style transfer, domain adaptation, and creative AI.
    ğŸ“„ [arXiv:1703.10593](https://arxiv.org/abs/1703.10593)

---

### ğŸ¨ Discrete Latents & Compression

* **VQ-VAE (2017, 2019)** â€“ *van den Oord, Razavi et al.*

  * Introduced **vector quantized autoencoders**, enabling discrete latent representations.
  * Key stepping stone for **DALLÂ·E** and **discrete diffusion models**.
    ğŸ“„ [arXiv:1711.00937](https://arxiv.org/abs/1711.00937)

---

### ğŸ–¼ï¸ Text-to-Image Breakthroughs

* **DALLÂ·E (2021)** â€“ *Ramesh et al., OpenAI*

  * First large-scale **text-to-image transformer**, built on VQ-VAE.
  * Introduced the world to generative **multimodal art** at scale.
    ğŸ“„ [arXiv:2102.12092](https://arxiv.org/abs/2102.12092)

* **CLIP (2021)** â€“ *Radford et al., OpenAI*

  * Jointly trained on imageâ€“text pairs, aligning vision and language.
  * Became the foundation for text-guided image generation (paired with diffusion).
    ğŸ“„ [arXiv:2103.00020](https://arxiv.org/abs/2103.00020)

---

### ğŸ“· High-Fidelity Image Models

* **StyleGAN2/3 (2019â€“2021)** â€“ *Karras et al.*

  * Improved realism and stability of GANs.
  * Still widely used in media and entertainment.
    ğŸ“„ [StyleGAN2 arXiv:1912.04958](https://arxiv.org/abs/1912.04958)

---

### ğŸ“š Scaling Laws & LLM Advances

* **Scaling Laws for Neural Language Models (Kaplan et al., 2020)**

  * Showed that model performance scales predictably with data, compute, and parameters.
  * Justified the move toward **GPT-3/PaLM-sized models**.
    ğŸ“„ [arXiv:2001.08361](https://arxiv.org/abs/2001.08361)

* **PaLM (2022)** â€“ *Chowdhery et al., Google*

  * 540B-parameter LLM with state-of-the-art reasoning and few-shot ability.
  * Introduced **chain-of-thought prompting**.
    ğŸ“„ [arXiv:2204.02311](https://arxiv.org/abs/2204.02311)

* **LLaMA (2023)** â€“ *Meta AI*

  * Released efficient, open foundation LLMs.
  * Sparked the **open-source LLM ecosystem** (Alpaca, Vicuna, etc.).
    ğŸ“„ [arXiv:2302.13971](https://arxiv.org/abs/2302.13971)

---

### ğŸ” Visionâ€“Language & Multimodal Expansion

* **BLIP (2022)** â€“ *Li et al.*

  * Advanced **image captioning** and visionâ€“language pretraining.
  * Important for grounding multimodal models like BLIP-2 and Flamingo.
    ğŸ“„ [arXiv:2201.12086](https://arxiv.org/abs/2201.12086)

* **Flamingo (2022)** â€“ *DeepMind*

  * Large multimodal model (text + vision) with few-shot learning.
  * Precursor to models like GPT-4V.
    ğŸ“„ [arXiv:2204.14198](https://arxiv.org/abs/2204.14198)

---

## ğŸ“ Bonus Timeline

* **2017** â€“ CycleGAN (unpaired image translation)
* **2017â€“19** â€“ VQ-VAE (discrete latents â†’ text-to-image foundation)
* **2019â€“21** â€“ StyleGAN2/3 (photorealistic synthesis)
* **2020** â€“ Scaling Laws (theoretical justification for giant LMs)
* **2021** â€“ DALLÂ·E, CLIP (multimodal revolution)
* **2022** â€“ PaLM (massive scaling + reasoning), BLIP, Flamingo
* **2023** â€“ LLaMA (open LLMs democratized)

---

ğŸ‘‰ Together with the **core canon list**, this bonus set shows **how the field diversified**:

* VAEs â†’ probabilistic inference
* GANs â†’ adversarial sharpness
* Transformers â†’ unified architecture
* Diffusion â†’ state-of-the-art generation
* * These bonus works â†’ **domain transfer, multimodality, scaling, and open-source ecosystems**.

---

The following are **foundational generative AI papers** that **directly set the stage for â€œAgentic AIâ€** (systems that donâ€™t just generate content but also *act, reason, and interact*).

Agentic AI isnâ€™t one single invention â€” it builds on **generative models + reinforcement learning + tool use + multi-agent systems**. Hereâ€™s a breakdown of the most relevant *foundational papers*:

---

## ğŸ§© Core Generative AI Foundations Carrying into Agentic AI

### 1. **Attention Is All You Need (2017)** â€“ Vaswani et al.

* Introduced the **Transformer**, the backbone of LLMs.
* Without it, there would be no GPT-4, Claude, LLaMA, etc.
* **Agentic AI relies on transformers for reasoning and planning**.

---

### 2. **Improving Language Understanding by Generative Pretraining (GPT-1, 2018)** â€“ OpenAI

* Introduced **pretraining + fine-tuning paradigm**.
* Foundation for LLMs that later became *agents capable of reasoning across tasks*.

---

### 3. **Language Models are Few-Shot Learners (GPT-3, 2020)** â€“ Brown et al.

* Showed **emergent in-context learning**.
* The â€œfew-shotâ€ capability is crucial for agents: they can adapt on the fly without retraining.
* Sparked the idea that **language models could serve as reasoning engines for agents**.

---

### 4. **Learning to Summarize with Human Feedback (OpenAI, 2020)**

* One of the first demonstrations of **Reinforcement Learning from Human Feedback (RLHF)**.
* Made LLMs *align with human intent*, which is essential for **agent trustworthiness and control**.
* Direct precursor to ChatGPT.
  ğŸ“„ [arXiv:2009.01325](https://arxiv.org/abs/2009.01325)

---

### 5. **Training Language Models to Follow Instructions with Human Feedback (InstructGPT, 2022)** â€“ Ouyang et al.

* First **instruction-following LLM** trained with RLHF.
* Made models behave more like *cooperative assistants*.
* This is where the transition from *â€œtext generatorâ€ â†’ â€œagent you can directâ€* begins.
  ğŸ“„ [arXiv:2203.02155](https://arxiv.org/abs/2203.02155)

---

### 6. **Language Models are Few-Shot Learners at Reasoning (Chain-of-Thought, 2022)** â€“ Wei et al.

* Introduced **chain-of-thought prompting**, letting models reason step-by-step.
* Critical for *planning and decision-making in agents*.
  ğŸ“„ [arXiv:2201.11903](https://arxiv.org/abs/2201.11903)

---

## ğŸŒ± Early Agentic AI-Specific Papers

### 7. **Language Models are Few-Shot Learners at Tool Use (2022â€“2023)**

* Papers like **Toolformer (Schick et al., 2023)** and **ReAct (Yao et al., 2022)** showed how LLMs can:

  * Call external APIs, calculators, search engines.
  * Reason + Act interleaved (ReAct framework).
* These are foundational to **AI agents that browse, code, or automate workflows**.
  ğŸ“„ [ReAct arXiv:2210.03629](https://arxiv.org/abs/2210.03629)
  ğŸ“„ [Toolformer arXiv:2302.04761](https://arxiv.org/abs/2302.04761)

---

### 8. **Generative Agents: Interactive Simulacra of Human Behavior (Park et al., 2023, Stanford)**

* First system showing **autonomous multi-agent societies** powered by LLMs.
* Agents remembered, reflected, planned, and interacted with each other in a simulated town.
* A true **prototype of Agentic AI**.
  ğŸ“„ [arXiv:2304.03442](https://arxiv.org/abs/2304.03442)

---

## ğŸ“ Summary â€” â€œFrom GenAI to AgentAIâ€

* **Transformers (2017)** â†’ scalable reasoning engines.
* **GPT series (2018â€“2020)** â†’ general-purpose generative pretraining.
* **RLHF (2020â€“2022)** â†’ aligning LLMs with human intent.
* **Chain-of-Thought (2022)** â†’ enabling reasoning.
* **Tool Use (2022â€“2023)** â†’ giving models ways to *act*.
* **Generative Agents (2023)** â†’ showing autonomous agent societies.

---

ğŸ‘‰ So: **VAEs, GANs, and Diffusion** are central to *generative modeling of content*.
But **Agentic AIâ€™s foundations come mainly from LLM + alignment papers (Transformers, GPT, RLHF, CoT, ReAct, Toolformer, Generative Agents).**

