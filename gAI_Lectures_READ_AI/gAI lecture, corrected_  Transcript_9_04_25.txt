gAI lecture, corrected. 
Thu, Sep 4, 2025

0:01 - Conference Room (Hong Qin) - Speaker 1
Okay, good evening. Let's wait a few seconds to see whether there are more students. Very good evening. OK, just a few bookkeeping. So I posted the lecture one video on Canvas. Also look at some of your self-introduction videos. Some of you did excellent job, but some of you just have trouble to apparently even produce a video. So it's a good exercise that it'll warm up. At the time, I guess when you actually have to produce a presentation video and then at least you are prepared. I think I also gave a data camp assignment. Oh, I see, you also need to sign in. Let me see. Yeah, please also sign I mean, Yeah, it should be more than six students, I guess. I'm going to just let So the next assignment deal will be to pick a primary research paper for presentation. Pick at least two, but one of them can be your own research project related to generative AI. But if you don't have one, then you have to pick two primary research papers. Question. Yes, question, question about, about our own project.

4:05 - Conference Room (Hong Qin) - Speaker 9
So like, does this, are we allowed to pitch a brand new project that we haven't done before? No, that's not it.

4:19 - Conference Room (Hong Qin) - Speaker 1
You have to present something with a result. It's not about a new project. That's interesting. I mean, you could pitch a new project to your other classmates, whether they want to work with you or not. I guess we could have some random breakout room, or you could try to introduce your project idea to other students, whether they want to join. About the presentation itself, if it's a new project, obviously you have an input or done anything about it. That's not a good topic. It's a proposal. Okay. I get it.

5:05 - Evan H. Mulloy
Okay. All right. Thank you.

5:08 - Conference Room (Hong Qin) - Speaker 1
Primary research paper is different from a proposal. Okay. I guess we're, that's only eight students today. I'm going to, yeah, maybe there's a lot of AI note occur here. I guess in that case I'm going to go to the syllabus quiz. Still a few students haven't taken it. This one most people down there. So, I posted a video. I guess I should have called it.

6:02 - Unidentified Speaker
Yeah, that is OIMU.

6:06 - Conference Room (Hong Qin) - Speaker 1
So, today it's the Variational Autoencoder. That said, even the books, at least chapter three, I guess for structuralism, we still live there. But what I am going to go over is the, again, it's actually the primary research paper actually proposed the variational autoencoder under the subsequent tutorial. And also I give you actually a detailed proof.

6:45 - Unidentified Speaker
Hold on.

6:46 - Conference Room (Hong Qin) - Speaker 1
A-V-E, V-A-E, I typed it wrong, sorry. Did I actually mention it? It looks like I saved the PDF wrong, that's why. It's V-A-E, V-A-E tutorial, not A-V-E tutorial. It's hard to change it now since that PDF had a typo there. I'm going to change that after class, I guess. Actually, maybe I can do it now. Hold on, let me see. Yeah, I have to upload a new. Okay, so now, so I'm going to first start with original paper, the VAE paper. If you click on that archive itself, it actually take you to that 2013 paper. Surprisingly, this is archive pre-print. This actually never peer-reviewed or formally published. You can actually see it. Interesting, it posted before holiday, just before holiday 2013. Well, in fact, it updated almost before the Christmas. Well, is that Christmas Eve? People walked right hard on this, I guess. And then after Christmas, New Year, quite a lot of update, and then it just dormanted for a long time. And then now also updated a new one, 2022, but the people just signed out the 2013 archive. And this is actually a bit surprising given that If you just put the archive paper, nowadays at the university, you won't probably get a good annual evaluation. But apparently these people, I guess it's a good work and they just put it in the archive and never bother to formally publish it. Yeah, that's it. This is a very famous paper for variational autoencoder. But I'm going to use my version, which has my notes on it. So I'm going to stop my laptop and share with my iPad. Oh, I'm good. Am I recording? Yes, I'm recording it. Oh, some people say you cannot find the room code. I guess the room code is... Let me see, how do I share my iPad. OK, I think I found it. Yes, I found that out. Okay, but this is two page. Okay, I had to flip my iPad to make it one page. So this is the auto-encoder paper, variational auto-encoder paper, but the paper actually is much more general. And variational autoencoder, apparently, is just one special application of this method. And the entire method is actually called something called autoencoding variational Bayes. And this is, I guess that's Danish, University of Amsterdam. Actually, I have no idea how to pronounce this. So this, what they actually introduced is a so-called stochastic variational inference learning algorithm. But what they do, this is actually not their own invention. What they really made a big deal is this, what they introduced the so-called re-parameterization trick. This is a really hard word to pronounce. So that's a good trick, and then also proving the lower bound of this. The introduction is kind of generous with acronym all over the place, but the method itself is here. This figure one actually listed the entire key idea of method. Those dark lines, those dark lines like... Oh, it doesn't... I could add a highlight. That's not... That's blue. I need to... Yeah. Those are the dark lines, and those are dashed lines. So apparently the darker lines, those are the actual, the so-called real world thing we want to study. It's a generative process, but usually this process is either intractable or unknown. So it has some X which is observation, we can measure it. And you can also, it probably doesn't go from the model to the data itself, right? So it usually have some intermediate step. In this case, we call the latent step. And for example, you may, one of the, let's say one of the example we can I'm very familiar with this. You have a child, let's say a baby was born. A baby was born, it has the genes from its mom and dad. The baby is going to grow up to a person, but with so many mom and dad, there's a lot of babies, and you have all kinds of people you can model. But in reality, do those genes just directly go to all those different people? It's not. There's a lot of process, development, education, environment, language, right? There's many languages in the world. So that's one example. For example, voice and text. There are many texts, alphabetic letter, even A, B, C, but I'm pretty sure in Danish they pronounce a little different. In French, German, they probably all pronounced a little different. So you have that model. You have that model. We have this model. But we have the X. The actual process, this Z, often the key to explain the difference in those X. So even though we have those alphabetic words, but they are speech will be quite different. So this z is really one of the key to explain why there are so many observations. But unfortunately, the z is really hard to model in many cases. And this paper will say, well, the z may be either intractable or unknown, but we can approximate it using, that's a theta, right? So we use this Greek letter. I often pronounce it phi. I'm not sure what's the right way, but either phi or phi. And we can use those to approximate it. Now this idea is also not new, right? So there's many, in fact, the entire, that's how the science has always to be. There's a lot of unknowns in the world the world maybe operate on its own. And we trying to use our, basically, this is our hypothesis in a way, right? We think, we humans propose a lot of theories, try to explain those. Everything looks miracle, but if you explain it, you say, oh, this is just science. So, okay. So this is not really new, but that's what they put this graph model, which is computational, and then try to give a mathematical framework to explain it. And this is a very nice theoretical approach. So they actually describe the problem detailed at the bottom. That's the graph, and the bottom, they say, Well, we observed a lot of X from the first one to the bigger N, which is the last one. So N is basically sample size, right? And there are also what they call independent, RID, independent identically distributed. I forgot which one is independent, which one is identical, but anyhow. They all follow the same distribution, and they're either continuous or discrete. And basically, x is the observation. And x came from z, which is the n of 0, which is latent. And in computer science, we'll call that the latent variable z. P is generated from the real-world method Theta. We actually don't know what Theta is. And we can call that conditional. Whoops. Sorry, this is my iPad. I just touched on something. I don't know. Okay. So it comes from this P Theta distribution, which we have no idea what it is. We try to study it. We try to use phi to approximate it. So yeah, basically, unfortunately, this process is either hidden or intractable. And we just don't know. So it's either intractable or very large data set we don't know. And what they are trying to say, we try to approximate theta with what? With phi. So this is really a general question. In fact, many of the scientific research, including social science, you can all generalize this way. What that really means, we will come back. The variational autoencoder is just one special case but you can build a lot of other things on top of it or modify it. So why we go through the primary research paper instead of going over the actual code? Well, I will come back to it after this. But the main idea is that you want to learn the foundation, the core idea of a method, not just a programming, a library call, because those things will change. In fact, the textbook example is already obsolete. What's the point? If you just learn the syntax and run the code, things will change. But the key idea is where the innovation, the principle will come from The problem is not new, but the way they come up to address this problem is entirely creative. So this is a very short paragraph, but that's the key. This is really the key idea of the variational autoencoder. There's a few, yeah, this one, two, three, four, five, six, seven, eight, nine, 10, 11, 12, 13, 14, 15 lines. But these three equation is really the key idea of variational auto encoding. So, the idea is that we have the theta and the x, right? So, if you go back to the previous, you have the theta, theta is here, you have the x is there, and theta we don't know, but x is observable. We try to approximate the theta. But how can we approximate theta. And if theta indeed has a function, the idea is everything in the world we can use a mathematical function to describe, even though we don't know, assuming they follow some mathematical function. So that's, we have to start with that. But if you, if you are argue the whole world that things cannot be modified by a mathematical function, I agree, but then we cannot study it, right? So there are indeed many things probably will be a stretch to describe by what kind of mathematical function we know, but maybe in a hundred years there are new methods described, right? So you don't know. So I know, but let's just start with something. We assume that the mathematical function will describe the observation we are going to study, which is a good idea. Otherwise, there's no computer science, no data science, no AI, right? So, you have to assume something will describe something. So, that's an idea. So, how do we... So, there's an idea called a marginal likelihood. Likelihood, if you are not familiar, you actually can ask Chattisby to explain to you. He actually does a very good job. You can actually see almost like if you go through a mountain there, right? Imagine that's a density. And if you somehow put that mountain, press it into a plane, and all the mass will compress to one two-dimensional. And you basically compress those mass into the one two-dimensional. That's basically the marginal distribution. That's just my way to understand the marginal distribution. The marginal likelihood basically, yeah, I think on the first we went through the likelihood and the density. Likelihood is basically given the model what's the chance the model explain the data. Probability density is just the function, the density function. So the marginal likelihood is basically a density function for every data point you multiply. Since it's a log, is the summation, oops, since there's a log, it's a summation. So it's just for the marginal of all the data point, you summation it over every data point. So it looks complicated, but that's the very basic idea. But apparently given on that, it just say it's going to write in this way. There are two terms, the first term is what's so-called a KL divergence for the approximate from the true posterior. Notice this is a z, you know, and this is phi. We're trying to approximate it at the first term. And the second term is the so-called lower bound. And this, if there's ever no Nobel Prize. These two persons will get Nobel Prize probably from that. Actually, Nobel Prize never give it to the mathematician, unfortunately. This is a lower bound. This paper just started with this, but it's actually not easy to get this step. They just write it down like To actually understand this, this is why I gave you a tutorial, Mathematica Proof. But online, there's also many other tutorials to see this. But I'm going to show, hold on. Let me see, can I have data? Yeah, so let me see what I can. Okay, do you see the step-by-step tutorial? Okay, very good. So there's a list of acronyms, but if you quickly go over that, and they also explain what is KL divergence. KL divergence actually looks complicated, it's really straightforward. So it's basically the entropy from two distribution Q and P, right? And that two vertical line looks intimidating, but it's just a notation. It doesn't say anything. You just say Q versus P. It's just the way people, I guess. Yeah, unfortunately, it's a math chart to put everything condensed, so it's not very intuitive. But people just use that as a way to say KL diverges of Q versus P. But in reality, that's just what the Q versus the P, that's the log, compare the Q distribution with the P distribution, and then just average out over Q. So this is just what, so-called, I think it's called relative entropy. So if you have done a lot of cross entropy, it's very similar to that. So this is just entropy. And the VAE introduction, it's actually right here. Hold on, hold on.

29:50 - Unidentified Speaker
Yes.

29:51 - Unidentified Speaker
Yeah.

29:52 - Conference Room (Hong Qin) - Speaker 1
So, oh, this is it. Actually not. Yes. So, let me see. KL-division with a nil using Bayes' rule. So, the P, let me see. Oh, I missed And this had to be rewrite even more step. This is not what I wrote. Hold on. Yeah, this is supposed to be like this. Apparently that version Yeah, so it's actually all started like this. So we have that distribution we don't know, but apparently if you take it Function. Yeah, it's still missing a step. Okay. Sorry, I thought that I put it there. Not. But let's see, I guess I still have to put a more detailed The second one actually does has one, hold on. Let me see. Let me see whether I can have... Trying to see whether I found another... I can share the second... Document with you. Oh, yes, I found it. Okay, let me. I share this. This one, yeah. Now this, so you start with this P which we don't know, but we just, oops. Yeah, so you just, the so-called, you broaden at the top, add the two turns, this approximation, then you separate the two, since it's a log, you separate the two turns. The whole thing is very, algebraically is very rudimentary. So that's all it does. So you separate these two. So this is the original theta. We added a hidden variable first. This is the latent variable. After you add a latent variable, you add an approximation. Maybe this is And then you separate the two turns, right, since the log of the two multiplications can be separate. And it turns out the second turn is the KL divergence, and the first turn is the lower bound. Yeah, it is amazing the entire VIE approach is all based on very simple algebra derivation. So what this means is just the log term, you have a log A, log XY, and you can put A here, put A here, right? And then you just become two terms, log X over A plus log the other term A over Y. It is just that simple. And since the KL divergence is greater than zero, that lower bound, that lower bound, they also use something called Jensen's inequality. And Jensen's inequality basically means that For any function with the concave, the mean of the whole function is greater than the function of the mean, something like that. It's basically getting this, the lower bound. Yeah, so it looked like a bit intimidating how this function become the KL divergence. But in reality, it's just a few algebra rearrangement you get it. Now, notice by then, all of this actually is not tractable. Those still are the known problem all the science or data science have to deal with. So the key really comes from this so-called re-perimeterization trick. But what it really is, it actually sounds really cool, but it's not. It's also very straightforward. So I'm going to... Okay, I found that.

38:19 - Conference Room (Hong Qin) - Speaker 1
So it's the so-called re-parameterization.

38:24 - Conference Room (Hong Qin) - Speaker 1
Oh, it first came out so-called, it first need to first have the auxiliary parameter and then yeah. It first came out with this auxiliary variable. It's actually called a noise variable, but it doesn't have to be. But in the variation auto-encoder, it has to be noise, because it has to be continuous. But that noise is also because it found a Gaussian. So it may sound like a noise, but in many cases, Actually, it's not a noise, but okay. Anyhow, it's just semantic in a way. So, it's going to... Oops, let me see. So, in order to address the above method, usually it just cannot be computed, right? It's just impractical, right? It's actually admitted. Yeah, we have the formula, but it's impractical. Computed. This is really the key. They come out with the auxiliary parameter, epsilon? Is it epsilon, everybody? I'm going to call it epsilon. Yeah, that's right.

40:01 - Anton Rasmussen
All right.

40:02 - Conference Room (Hong Qin) - Speaker 1
Thank you. Yeah. So, it's going to, yeah, this is a very interesting. It's going to assume these are, yeah, guy are really creative they're going to assume there is a function called g theta with the epsilon and then you just can can get the z from the from that and the epsilon you can't just take epsilon epsilon has its own function take epsilon from the epsilon function This is really quite a bold statement, right? So you already have a hidden variable z, and then look at where that z come from z is here, right? Z is here in the middle, z is here, whoops, z is there. Why is it not showing? Okay, z is here in the middle, but it's going to come out with another variable called epsilon to model z. This is a really bold statement, but very clever too. Then they are going to use that epsilon to help model z. Because the is the auxiliary variable. They're going to use the g to approximate it. In the variational autoencoder, they just assume this. This is really amazing. They just say z equal to mu plus variance n times epsilon, which of course is a univariate Gaussian. Clearly, that's a bold statement, but they assume, which is, I think I understand why they just put it on archive without peer review. They just say, well, we assume they can be approximated by that. In fact, this is not approximated, it's just equal. They're going to just use that linear equation to assume that is Z. But of course, in data science AI model, because Z is the latent layer, we just want to use a whole model to model X. As long as the model X, that Z, you actually can't really tell whether that Z is really the true or not, because it's all a latent variable anyway. Which means we can actually do a lot of other tricks, right? In fact, there's a lot of other tricks down about the z and the epsilon, how to model this. But of course, this is in 2013, this idea come out and it works. It's really opened the door for many other opportunities. By the time to first do this and actually show it works, it's certainly a big deal. But of course, you kind of wonder why, right? This is just kind of a why. Yeah, apparently, with a variational autoencoder, assuming that's correct, You basically, this look a lot of fancy, but that's just Epsilon. In this case, Epsilon is just the indicator. Basically, if it's one-hot encoding, just one, the matrix. Which is really amazing this actually worked. Yeah, and that's really the key idea what I found is this original 2013 paper. Okay, with this, we now go back to the books version or with a Python code. And we actually, hopefully can, Instead of looking at the code itself, hopefully you can have a more deeper understanding. It also lets you realize the most variation autoencoder people are using now with the Gaussian mean and variance, it doesn't have to be. It could be something else, even though it's popular, but it doesn't have to be there. That's the message I'm trying to tell you. It is there because Gaussian is easier to implement, but it doesn't have to be Gaussian. In fact, if you can come up with a better way to model this hidden variable, I'm sure you will make a lot of money. Or at least make your name of doing something good. Okay, so now I'm going to go back to the textbook. In a way, the variational encoder I also picked a primary research paper instead of going through the textbook. And hopefully this also gave you some idea what kind of a primary research paper you should be picking. Basically, it should demonstrate some foundation principle of generative AI. After reading those, you can actually come up with your own idea how you can modify those and even propose your own model to do it. In fact, with generative AI, people claim they will come up with a GPT model now with a PhD level intelligence. You kind of wonder if you just learn how to run some Python code, will you be replaceable or not? So you really need to come up with some idea what you can do now is not replaceable by AI or at least for the next 10 or 15 years. So, right, so, hold on, let me, I need to stop sharing from this and then sharing with my, okay, I don't see my Kindle book on this one. This is strange. I don't see my yes, someone. Question.

47:44 - Conference Room (Hong Qin) - Speaker 9
What if we find two papers where the generative AI gives very good results for tasks that can be done by humans? Is that okay?

47:57 - Conference Room (Hong Qin) - Speaker 1
Sure. Yeah. Thanks. But otherwise, I want to stop my iPad and go back to how do I do that? I'm going to Okay, I think I just stopped sharing Okay, I found it, okay. I'll just start sharing on my computer, that's it. All right, so the Boxed Variational Autoencoder is really practical. It actually started with a Python coding package, Keras, a very old Keras code. I didn't even bother to try to run it, honestly. I'm sure you you found out the right environment, you can still run it. But the book does have a good idea. It didn't start with a variational autoencoder. It started with the autoencoder first to tell you what is the autoencoder and then what is variational autoencoder. So the autoencoder is actually used, let me see, Let's see.

49:06 - Unidentified Speaker
Yeah, it's also started with a Python example.

49:11 - Conference Room (Hong Qin) - Speaker 1
In a way, it's good because it gave you a kind of a tangible way to show how things work. In fact, the book actually takes pride, say, in the beginning of the sentence, something It says, it actually quoted Richard Feynman, what I cannot build, I do not understand, or something. Let me, how do I find that one? It actually does, can I search it, Feynman? Yeah, see, it does, page 10, it does say, what I cannot create, I do not understand. So the book, in a way, takes stride to say, I'm going to do this, which I agree. But what I'm trying to say is that's not enough, because if you just write a Keras to a Veresh autoencoder, I'm pretty sure JCPT can do it very well as well. So what's the point that you can't do it, right? Oh, but I'm sorry. I'm auto-synced to each chapter, I'm not. Okay, now I come back to VAE. So, first start with the auto-encoder. In fact, I'm pretty sure ChaiGB can write this part very well, but it's still good for us to review this. Let me see. Very good. So, you have the encoder, decoder. So Z is a latent one. So you can think about encoder is theta, decoder is phi to approximate. But if there is no noise, they will be basically come back almost identical. But that Z, because it's compressed, I guess it's good to reduce some noise. So if there is no variational autoencoder, this just becomes denoising process, or signal compressing, because Initially, you're going to have a lot of the channel, but after you compress to Z, you have less dimension, so the signal is more compressed. In this encoder, it's not very complicated, just a convolution. Flattened, it becomes impressively just two dimensions. The encoder, it just compresses everything from 32 by 32 into what? Two, wait, is that just two values? Yeah, it doesn't look like two values, yeah, so. But it's a floating point, so actually, it's basically, yeah, two value floating point, so. It is impressive. Computer or autoencoder can do this. That's the implementation of the encoder in Python. Convolution, convolution, convolution, flat and dense. It's just a few lines. Then the decoder is the other way around, but it doesn't do the convolution. It is a convolution, but it's actually expanding. It's expanding back to 32 by 32. It also gives you some explanation of how the convolution transposed to expand the smaller pixel to more pixels. Okay, then in Keras, it's joining the encoder and decoder, and then that becomes auto-encoded. And this is a binary The input is just those images, very tiny images, 32 by 32, but they are labeled into, I guess, something like shoes, tops, pants. What's a jacket? I forgot what that label is. A shirt, top, trouser, or something. Yes, dress, coat, sandal, sneaker. Back, ankle boots, boot. Yeah, so those are the labels, but those are the input images. Yeah, encoder, decoder. Yeah, apparently those patterns on the clothes are noise. So, well, yeah. I guess if you call both of the top, then those are indeed a noise, right? So noise is really kind of subjective, right? You say something is noise, but if you choose another angle, maybe it's exactly what you want to study, right? If you want to design new clothes, of course, there's a noise, not noise anymore, right? So yeah, noise is kind of subjective. All right, so this is actually uh this is interesting is actually the latent space after your training you put the label on this latent space and then you realize oh yeah so those labels things with the same label tend to be connected on that in that latent space right which makes sense but then you do see some mixing here green and red green is the four to six red yes what kind of thing will make sure that four to six coat sandals shirt red is what nine boot boot on the sandals sometimes can be mixed I guess I don't know but yeah could be so back how come back be missed I'm just mistaken with sand. I have no idea, but anyhow, so yeah. I guess with the image is small, it is possible. And then this is actually interesting. If you want to generate the image, you run the sample something, which is this. 24 minus one. It still looks like a shoe. Is this sneaker? The input data doesn't have anything there. Very little, I guess. If you pick from that in the latent space, you do get something. Yes, someone raise your hand. Yeah. Professor.

57:04 - Conference Room (Hong Qin) - Speaker 6
So I thought like when comparing, like, like in classification task or so we didn't like with vectors, so it's high dimensional space. So, like, how are they, how are they visualizing this in two dimensional space, just distribution.

57:22 - Conference Room (Hong Qin) - Speaker 1
This is basically that the z, you're visualizing, this is the That's the Z latent variable. You have to compress everything into Z, which is two dimensional.

57:37 - Hamza Chao
Oh, okay. So, okay. Okay.

57:40 - Conference Room (Hong Qin) - Speaker 1
Yeah, that's the trick. Yeah. In fact, that Z is basically, if you recall the first chapter, sorry, I'm just flipping the book here. There is a manifold. You can kind of think that's a manifold. Without a manifold. Sorry, this is very Oh, yeah, here. You can think about that. That's very similar to this. You can think this is a Z and this the manifold. Yeah, you can you can. This is completely speculative, but that's the real example, right? So is there a way I can jump forward? Thank you for this. Sure. Sure. Yeah. Yeah, okay, good. I still remember it. Okay. So, yeah. So, in fact, they actually overlaid the actual image you can see. So that's the, oh, that's the anchor boot. Boot, boot. Then it become back. Yeah, I guess you see the boot is longer enough, it's closer to a back. Sorry, I shouldn't laugh. A lot. But that's what the image looks like, That's what this AI model learned. That a boot, if it's longer enough, will become a bag. That's why you go back to this, you see a mixing of these labels. That's how AI models learn things. I mean, of course, if you tell a baby, you probably can tell a boot from a bag, but the AI model learned it this way. So, yeah. So, AI is dumb in a way, right? I mean, it looks impressive, but it's really dumb. Which means I'm pretty sure if you have a better way to organize the latent space, I'm pretty sure you can come up with by the way, to tell a bag from a boot. So once you look at this map, you realize how oversimplified this method is. I mean, it is still amazing. In 2013, you can't do that, but it is actually, yeah. Oh, by the way, this is auto-encoder, not a variational auto-encoder. So the variational auto-encoder, instead compress everything into a two-dimensional, it's actually now, as we said, z equal to mu plus sigma, which is the variance, times epsilon. That's the variational autoencoder, the famous variational autoencoder, but it doesn't have to be that way. But we can implement it in that way, right? Z equal mu plus variance and modify You can consider the epsilon as a weight of importance. There. That's the variational autoencoder. The autoencoder, every data point goes to something, but the variational autoencoder is going to use the mean and the variance plus the auxiliary epsilon, which in this case is an indicator function. And then you try to explain what Gaussian distribution is, which basically is this bell-shaped curve. Well, it's two-dimensional, it's going to be a hill. If it's three-dimensional, it'll be... Actually, I'm not sure how to explain that. It's probably, if you think about it, you drop ink in the water, you can see the spread out, that's probably... If the water is a lot of ink spread out evenly, that would be closer to the three-dimensional. Oh, someone had, yeah, Evan. Bell.

1:01:55 - Conference Room (Hong Qin) - Speaker 9
It's still a bell shape, even in three dimensions, if you think about it.

1:02:01 - Evan H. Mulloy
Oh, really?

1:02:01 - Conference Room (Hong Qin) - Speaker 9
Yeah, a bell, like a, you know, church bell. It's still called a bell, I think.

1:02:08 - Conference Room (Hong Qin) - Speaker 9
I see, I see.

1:02:10 - Conference Room (Hong Qin) - Speaker 1
It depends on how you look like, It's a normal distribution, so it has to be the bell curve. Yeah, yeah. The curve, by definition, is two-dimensional. Right? If you have many things on two-dimensional... Okay. In a way, it depends on how you look at it. I guess if you put everything, put it on top, it's also not a bell. It's actually density.

1:02:56 - Conference Room (Hong Qin) - Speaker 1
Yeah, that's fine.

1:02:58 - Conference Room (Hong Qin) - Speaker 1
But let it go. Let's let it go because the curve is It's a bell-shaped curve when you force it on the curve. But in reality, it's just density. For example, electrons. Electrons more happen inside up. It's really not a curve. It's just more probability around that nucleus, right? If you look at electron density in the space, it's really not a bell, it's just more probability around the nucleus, right? So, anyhow, I'm going to just skip this because it's probably more than a bit distract from what we are trying to, but you can't, but in a way you can't force it as a bell-shaped curve. That's it, right? But in reality, it's probably not a bell shape. So, okay, but we can, it's also dependent on whether that Y-axis you are trying to represent But then if you put the density as the, in this case, density is probably that vertical axis with a perpendicular to the bottom space, and then it is the bell shape. But if you remove that one, and then exactly everything will fall down into that one two-dimensional plane. So I'm going to just skip. Get over now. So, okay. So, this, well, with the z which is equal to the mean and the variance, and then we can change the autoencoder into variational autoencoder by the mean and the variance. So, this is just assuming everything, yeah, It's easier to implement, I guess. Once we have the mean and the variance, we actually have to sample from that mean and the variance. Return the mean plus epsilon, so there, epsilon. That's the auxiliary parameter. We have to use. And the epsilon, it come from what? Come from the normal distribution as well. Epsilon, apparently sampled from the mini-batch. Every batch is sampled from Yeah, it's amazing this actually worked. Now, K is the, that's part the Keras library shorthand. Okay, so the encoder... How do I turn off the transcript?

1:06:16 - Unidentified Speaker
No.

1:06:17 - Conference Room (Hong Qin) - Speaker 1
That's the caption. Let's look at the encoder layer. This seems to be similar because the encoding still the same, right, so. Where's the decoder layer? Oh, and then it added the mean. The variance here. I am sampling the mean v I with the epsilon. Oh, in the function, in the sampling function here. That's the epsilon. So epsilon is entirely encapsulated by the sampling function. That's the sampling function. The sampling function and then the encoder. This is interesting. The epsilon is not even specified outside of the encoder. It's entirely, we don't even see it. Basically, it looks like the computer, the training process doesn't even learn from it or train from it. Now you have the mean and variance. Sampling. OK. In this case, the KL divergence is also have an analytical solution because, I guess, with a normal distribution as the approximation, you can actually write it down actual formula. In the tutorial and the PDF, they also provide an analytic solution to show how that KL divergence is there, why that is the case when the normal distribution is there. And we can't just that me, So. A decoder. I feel burned. That's just a sampling stack. I didn't find the decoder. Where is the decoder? Decoder layer, I did not see that. It seemed to be all about encoder. Oh, this is, okay. It doesn't show that the loss function has two turns. There's a reconstruction loss. There's a KL down. Divergence loss. The reconstruction loss is also, it seems to be just a cross entropy. Mean of loss of binary cross entropy. Then apply the gradient descent. It looked like this is just default decoder they are using. There's not any override on that. Oops. So so this must be mu and new latent space this will be the z now is just mean and variance so one one of them must be mean the other one must be variance and this one So the label again is put on the latent space. Well, you do see this is, oh, what's the difference? Closing type. Anyone find out what's the difference between I write on this figure oh we just zoom up zoom in zoom up yeah so this is bigger minus 3 to 3 this one is zoom in between 0 and 1 0 and 1 is that same yeah this is just zoom in red orange green yeah this just zoom in on a small section Is it a better one than the previous one? Phi and A It seems to be more mixing up, isn't it? It doesn't seem to be, it's more separate than the previous one. One. So if you just for classification, that doesn't seem to be improved. But since the goal is to generate a new data set, the trade-off, the classifications seem to be less effective. Before that, it's much more clean separation in the latent space. But with that, even though with the VAE, the classification separation clearly is not as good as before.

1:14:58 - Conference Room (Hong Qin) - Speaker 5
But of course- It looks like, Professor, it says the right-hand plot shows the space transformed into p-values.

1:15:06 - Conference Room (Hong Qin) - Speaker 1
Is that- Oh. I see, I see, I see. Thank you. Yep. Oh, it's not a, it's just not, normalize into p-values or transform into p-values. I see. Okay. Thank you. No problem. So it's just somehow to transform this left or right. Okay. That's basically the probability, I guess. I guess probability is a Yes, this is actually the entire transformation of the left to right. So it's just a scale I changed from the mean and the variance into probability. I guess the textbook does give you some good hands-on exercise. This is a Kaggle data set. There is a so-called celebrity data set and apply that for the variational autoencoder. So variational phase encoder. Just model the face. Yeah, so this, I guess if If you are interested, you can try to run that code. You probably have to modify. I guess the easiest way is just to create a virtual environment, put the old version of Python and put a Keras there. That way, you don't have to modify the code. Of course, if you really want to challenge yourself, you can rewrite the whole thing using what, PyTorch? But then of course, the question is how do you get that data? You probably still have to, where the data come from The data, oh, actually data is just download. Yeah, so it's fine. So data apparently is just batch downloaded. So, yeah. Then you have to rewrite some other code, use the old Keras code. That actually could be a good course project if someone wanted to do that. But this is just first chapter. I'm pretty sure there are many other more interesting topics you want to work on. With the latent space is there and we'll know the trick. Or something interesting you can do is so-called a phase morph, right? With the, if it's inside a latent space, and if you have the latent space, if you know one label is there, the other label is there, and you just gradually mixing these two labels and change it to something else, right? Clearly, if it's shoes and handbag, if you change, nobody is impressed. But if you change someone's face from one person to another person, it looks more entertaining. But the trick is still the same. Basically, you modify the latent value between the two labels. And that's actually, if you think about it, it is a very good contribution, right? Before you have to do that, it's all pixels. Now you actually do have something to manipulate, right? So it's a very simple formula here. Yeah, that is quite amazing. Now, This is just variational autoencoder. But if we look at the, let me see. If you look at the other PDF, hold on, let me see. There's recently a lot of new development on this. So, I'm trying to see the, the new tutorial. So the author apparently gave another more in-depth. This is more after more than 10 years, they gave another PDF on this. That one actually does have some very good figures. I want to go back on my iPad to share that.

1:21:28 - Unidentified Speaker
Is this sharing?

1:21:33 - Unidentified Speaker
No, it is not.

1:21:38 - Conference Room (Hong Qin) - Speaker 1
Oh yeah, I am sharing it now. Okay, yeah. So this is the new paper from these two guys. I guess one of the person still stay at the university. The other guys are now work at Google. It's not surprising, is it? So yeah, so they actually, indeed, they try to emphasize variation autoencoder is the integration of graph model and the deep learning, which if you go back to the previous paper's figure, you'll understand why they want to do this, right? So what they're trying to say, whoops, Sorry. Yeah, the thing on iPad, if I mis-touched the wrong thing, everything changed. Sorry. Yeah, so again, I'll try to emphasize, we have that, the parameter theta, but everything will be amplified through this latent space. But what we can't observe is the data, right? So this is what we can see. We observe the world, we see the data. But we're trying to infer those myths of the world, or the principle of the world, depending on your preference, I guess. And so we train the model using phi to approximate. And once we train that with the hidden space, we try to decode it. Actually, we should never influence the theta. But unless you are seeing Gaussian distribution, swing rate. Actually, there's both steps to do this. But this is, I guess, clear enough. Remember in their first paper, they just put a statement. So we have the log p marginal, and then they just write down the two-point They just write it down. Apparently, they think quite obvious from the 2.5 to 2.8, but that's actually the difference. You can actually see the... Oh, here. The re-parameterization trick. Called a parameterization trick. They do also use a graph chart to explain it. The amazing part is this auxiliary parameter. Without it, it doesn't work, but it seems the whole training process doesn't even affect it. It's both shocking and impressive at the same time. But there are actually a lot of new methods coming out of this. Let me see. They also show this is very close to the maximum likelihood approach, which is very classical probabilistic method to estimate parameter, but they actually try to show this is very close to that method. They actually show a lot of... You have one auxiliary. Apparently, you can have multiple auxiliary. Parameters. You can even have an auxiliary latent variable. In reality, we never know there's just one latent space. There could be two. So you can use two latent variables. In fact, if you really expand on this, people still say there's a lot of dark matter in the universe. We just It's probably there, we just don't know. In fact, many of the things the universe is operating is because it's dark matter there, but it's really hard to describe it. The point is, we always use z as the latent space, but what if there is another thing we don't know yet, that is u, another latent space there. It's possible. Once you put there, then there's actually a lot of other opportunity that can be opened up. There's also another way, so-called flexible posterior, which is through what? You can actually, apparently, if one auxiliary parameter doesn't work, you actually can have multiple. This is really auxiliary parameter on steroid, I guess. You can have a whole series of auxiliary parameters. Then there's So it's actually expanding even more to auto regression model now. So now you have the auxiliary parameter. What are those parameter? I haven't Read this part yet. So apparently, now you see this multiple accelerators, so auxiliary parameters. I haven't Read this part. What I'm trying to show is there's a whole lot of enterprise built on this basic variational autoencoder. It's actually not that simple, which means you can do a lot of, once you know the parameter, you can do a lot of things. Right. Inverse auto-regressive flow. Yeah, you see this? 0, epsilon 1, epsilon 2, all the way. So, gee, you can build up the whole thing. Remember, this all comes from the Bayesian inference. The Bayesian inference, the very simple trick, you just multiply. You put one in denominator, the other one in numerator, and then you can multiply as many times as you want. In principle, they're all the same thing. Yeah, this What I'm trying to show is that, yes, we have the Python code, but remember that normal mean and sigma doesn't have to be there. There are many ways to rewrite the whole architecture to generate a new model. You can have a multiple latent variable, clearly. Gee, this is a new paper. I haven't Read it. You can actually put some more so-called vectors. You can put a smile vector, produce a range of emotions on top of the face. There. This has actually already touched a lot of other chapter as well, but at the bottom is just... I guess that's it. The key is what? Latent space, and then with the accelerate parameter to so-called re-parameterization trick, but it's really not. It's just a In fact, that's what it is. You just use a simple model trying to explain the latent space. If you actually indeed have a way to explain that latent space, you could just use a different model. I'm going to stop sharing. Let me see some other... Let me see whether people that are checked in yet. Yeah, I have 10 students checking. Officially it's 11. But there's someone still haven't done anything on Canvas, so I'm not sure whether that student is dropping a class or not. So anyhow, 10 is enough. Okay, let me see. So I'm going to go over a student self-introduction. Oh, I don't want to show the grid. How do I show that? How do I do that? I'm going to, okay. I'm trying to pop out a video and then just show that video. Okay. Some student does a very nice self-introduction. In fact, even in the presentation, say, talk to the class, but unfortunately, that video only I can see. So I'm going to try to play some of the video. Okay, so this is Professor, I'm still looking at a textbook.

1:33:22 - Conference Room (Hong Qin) - Speaker 9
Yeah, I'm trying to stop that.

1:33:27 - Conference Room (Hong Qin) - Speaker 1
Professor, so this is my introduction for the class.

1:33:32 - Conference Room (Hong Qin) - Speaker 6
So my name is Mohammed Hamza Chao. My first name is Mohammed Hamza, but you can call me I'm a master's student in computer science, Old Dominion University. I'll be graduating this semester. So yeah, I'm originally from Morocco. I was born and raised there. I had my bachelor's degree in computer science from even to finally University in Kenitra. I moved to the US in 2022. And yeah, I was just working for one year. And the next year I decided to pursue my master's degree. I speak Arabic, French, English. It's mainly because they teach us French in school in Morocco. And Arabic is our main language. So yeah. So right now, I work as a research assistant for VMask. We work on some projects. For example, one of the projects was for the military. It was a systems engineer, but they had some coding because they had a simulation environment. It was done by Python. So yeah, one of my technical strengths is Python, PyTorch, Transformers, Spark in the SQL. One of my recent projects is a Steam Sense. It's a recommendation system for Steam. So based on your Steam profile and how many games you played, it will try to recommend the games that you played. And another one was a taxi destination in a Portugal data set. Yeah. So I'm taking this course, um, I'm not going to say it's like transition, but, um, because like mainly my, my, uh, like, uh, throughout my whole time in my master's, like I was doing a lot of data science, data analysis, and I actually did like one NLP class and I actually liked, um, like the theory behind it. And then it was, um, my final semester and I saw that there's this AI role that they need, you know, because I realized actually like after doing some interviews for like internships or jobs, I noticed that a lot of them, they ask about NLM, like AI experience or knowledge of AI. So I realized, you know, this class will help me a lot, like once I graduate to find a job in AI engineering or data science. And yeah, I want to, you know, at least like build a project, mental application. So it will go to my resume, increase, you know, my chances of getting a job after graduation. So mainly, so right now I'm taking the Landgraf course. It's a free, it's, it's a free course, uh, given by like chain, uh, a company. So yeah, one of the, you know, to learn more about it, you know, maybe work on a couple of projects. So I want, I also want to do, so, um, I want to do like a project that includes, because like graph is maybe for like agents, multi-agents applications and like chain is for the LLM.

1:37:25 - Unidentified Speaker
applications.

1:37:25 - Conference Room (Hong Qin) - Speaker 6
So I wanted to do something that has both. So I'm trying to figure out any projects that I can do, you know, like shade with line graph. And, yeah, basically want to, you know, learn more about evaluation of LLMs, fine tuning, deploying the app. So, yeah, this is my contacts information, my email, and yeah, when I reach you on anything, you know, better.

1:38:01 - Conference Room (Hong Qin) - Speaker 1
Okay, thank you so much. Let me see.

1:38:15 - Unidentified Speaker
Hey, my name is.

1:39:00 - Conference Room (Hong Qin) - Speaker 3
And I'm going to present a presentation about myself and introducing me about my background and my interest in taking the course generative AI. And my UN is 0118602. And my email is on the screen. And to start off introduction, my full name is Aditya Choudhury Doppalpudi, but I would be preferred to call Adi or Aditya, whichever is comfortable for you guys. And speaking of myself in terms of academic, I'm a master's student at ODU and I'm pursuing computer science a master's student. And I just started my second year of master's, so there's one more year ahead of me. And where am I from I'm from Hyderabad. That's in India. That's a city in India. And that's where I did my undergrad. That was in Electronics and Communication Engineering in the College of J&T, University College of Engineering, Science and Technology. And coming to the relevant background in AI or computer science in general, I completed the courses in data science and analytics and utility-scale quantum computing last semester, which I find might not be directly helpful for gendered AI, but would be somewhat interrelated or potentially related. And upcoming years. Data Science Analytics is very much in combination with AI, obviously. And when it comes to my coding experience, I've experienced coding in Python, C, Java, HTML, CSS, Bashel scripting, JavaScript, Node.js, MySQL. Some of the skills that are not coming to my mind right now. I'm also taking the course Data Analytics and Big Data that's being offered the same time as the course Generative AI, which is more in terms of big data operations and also has a little bit of AI included in it. The reason for me to take this course is I want to understand how AI models function, at a deeper level. I know how they operate, like how tokens function, but I want to be able to know them at like a technical level and also be able to, you know, build some kind of application using them. Using either chat GPT, like OpenAI, SAPI, or Hadoop-based models that are already existing. I already used some models, like a few million parameters, a few hundred million parameter models. I didn't achieve much, I just tried them out. And basically I want to learn how to utilize existing models, that's like the latest JGPT-5 and so on. And I want to be able to be on top of the current trends and the progressions that are happening in generative AI and AI as a whole. And to make myself more relevant and necessary than what I would be left behind us. When it comes to the things I would like to learn from this course, I would like to be able to build a small application using APIs from OpenAI or Gemini or whatever that's available to me and or try out them all and build a small application to get some experience. I'd also like to work with PyTorch or TensorFlow to try and have a little bit of experience in deep learning aspects of generative AI. I would love to explore the fundamentals of transformers, Gen A and Gen Z. I don't have a lot of knowledge in any of those, so there's a lot I need to learn about those, and I feel like this course would be perfect for that. And I would also like to get some hands-on experience while learning all of these by kind of or doing some project. On top of that application or something based on the research that I just, you know, researched and presented for the initial presentation task. So yeah, the main thing I would like to gain from this course would be a lot of experience. And the project that would be useful for my resume and for myself like skills level. To show at what level of skill I am from the before the course and after the course and I'd also like to learn and keep up with the advancements that are happening in AI and generative AI which I feel like are very impactful in the world or yeah which are very impactful in the world so it's essential to keep up with them and I'd also like to keep an eye out for research that talks That is basically integration between quantum computing and AI. So that would be a really good goal for me. And that's it for my background and what is my purpose and what do I hope to learn from the scopes. And if you have any doubts, please reach out to me.

1:44:39 - Conference Room (Hong Qin) - Speaker 1
Thank you. I'm trying to find our next student. Yeah, apparently a lot of you, I would say quite some of you just gave the PowerPoint. I did ask you for video, not PowerPoint. I said PowerPoint-based video presentation. So if you just gave a PowerPoint, which means you did not follow instructions. Oh, yeah, someone raise their hand. Yes, it will be professor. I upload my presentation, and the video is inside in the presentation.

1:45:21 - Brian Llinas Marimon
So you need to download the PowerPoint and then put it in presentation mode.

1:45:29 - Conference Room (Hong Qin) - Speaker 1
Let me explain to you, you gave me a PowerPoint, it's not That's not a video. You say video inside, that's not a video file. That's not a video presentation.

1:45:41 - Brian Llinas Marimon
Is that making, is that clarifying?

1:45:44 - Brian Llinas Marimon
I mean, what I did is add the video in the presentation so you can see my face, what I'm talking and those stuff.

1:45:58 - Unidentified Speaker
That's not a video presentation. Okay. Okay. Hello, my name is Evan Malloy. I'm getting my master's in computer science.

1:46:27 - Conference Room (Hong Qin) - Speaker 9
Let me I'm from Frontwell, Virginia. I've been programming since I was 12. I graduated from ODU in 2021 with highest honors. I now work as a software engineer for a consulting firm that does cybersecurity. I've taken some AI courses in the past, namely courses that deal with machine learning and a little deep learning here and there, mostly classification and regression, as well as a little bit of some deep Q learning and reinforcement learning. But I want to be able to learn to use generative AI. I want to be able to create generative AI models. I use generative AI daily to help write code for me, you know, low-level code, while I focus on the high-level architectural things. My other interests include Lego, 3D modeling, photography, and science fiction stories. You can see my GitHub link there. Why am I taking this course? To finish my master's, which is obvious, but also to gain knowledge that will help me give myself job stability and advance my career, especially since AI can replace a lot of the work that used to be done by entry-level programmers. I also wish to learn more I find genuinely interesting. I wish to learn how to write applications that use generative AI models. I wish to learn to create generative AI models and fine-tune them, maybe even create my own proprietary models. I also wish to understand the field of generative AI in terms of academic research so that I could do research of my own.

1:48:24 - Conference Room (Hong Qin) - Speaker 1
I look forward to working with you all. We go to the next one. I hope there's a better way to present this, but I haven't figured out how to... Apparently, some of you shared the video. Apparently, I just cannot see it. Oh, there's a new one. Okay, hold on. Oh, this is new. Okay, this one, I cannot be anonymous because I had to sign in. Okay. All right, that's fine. Okay, this one, I had to sign in to show this. Hi, good afternoon.

1:49:55 - Conference Room (Hong Qin) - Speaker 7
My name is Sergio, or Sergio Payacenita, Sergio in English, I guess, and just a small presentation to kind of introduce myself to the class. So I'm a PhD student under the ECE department. My focus is machine learning and computer vision, and I'm working under Dr. Chen. This is my fifth semester, but I didn't have a master's degree. I'm doing both. So I'm actually taking my diagnostic exam this semester. So I'm probably going to be here for another three years at least. I'm originally from Barcelona, Spain, but my family is from Zaragoza. So here's Barcelona, Madrid, which is most people know those two cities. So we're right in the middle. We're fourth largest city in Spain. And I was born in Barcelona just because my parents lived there. My dad was working there and I was just born there. But again, like culturally, like I feel like I'm more from Zaragoza just because that's where all my family is, our soccer team, like food-wise, music-wise. But yeah, we moved, we did move a lot. When was

1:51:04 - Conference Room (Hong Qin) - Speaker 1
I lived Germany for a while and then when I was around nine years... I can't hear the audio.

1:51:22 - Conference Room (Hong Qin) - Speaker 7
I made a lot of friends. Different culture, but same language, which was weird, but it was really fun. And then I moved to Miami for my bachelor's. So FAU is a school I was in. It's like the research public university down there. Moved back to Spain for six months and then moved to Norfolk or Virginia Beach for my doctorates. My family did move here as well from Mexico. So now I'm kind of living with my family again, which is nice. And yeah, that's just kind of where I come from So some background. So why I took this class, I just want to get a bit more hands-on experience with generative AI and just AI in general. I do work a lot with machine learning and with deep learning. But it's different. It's like we're kind of neighbors, but not right there. And I'm working with apps, UIs, and stuff like that, which would be a really good kind of place to integrate generative AI and some language models into, so I just wanted to get that first contact through a class and some projects. Again, AI background, not really much. I do have some large projects with machine learning models, neural networks, stuff like that, computer vision sensors, but not generative AI or AI as such. I did take a class in interpretability for AI, but we didn't really dive into AI itself, which is kind of looked at results, models, and how to interpret those.

1:53:02 - Unidentified Speaker
Some interesting facts.

1:53:03 - Conference Room (Hong Qin) - Speaker 7
I really like to play instruments. I played saxophone and piano growing up. I don't have any of those instruments at the moment, but I did start playing guitar last year. I just bought a electric guitar and acoustic guitar, have both of them, and just, you know, kind of starting to learn those and playing music again, which I really missed. And as of last February, I started learning to play ice hockey. It took like two months of skating classes, and now I'm starting to learn how to play hockey. It's a tough sport. It's hard, but you know, I'm learning. And I'm a big soccer fan as well. This picture on the right is me in FedEx Office Field, which was nice. This is around two years ago. And fun fact that That stadium is actually turned down right now because they're building a new one.

1:53:54 - Conference Room (Hong Qin) - Speaker 1
So yeah, that's just a little bit about me. All right, thank you. Okay, let me go. But, uh.

1:54:46 - Conference Room (Hong Qin) - Speaker 4
Hello. Good afternoon, everyone. My name is Abdukufor Recibo. You all can call me. And potential background relevance that I bring to this course as well. And also what I hope to learn during and by the end of this course. For my background, like I said earlier, my name is Abdukofor Raji, or you all can call me Gaffy. I'm just entering into my second year of PhD program starting this fall. My major is computer science and my country of origin is Nigeria. Prior to this, the Masters in Information and Communication Sciences at Ball State University in Indiana. So my motivation for taking this class, my motivation for taking this course cuts across three separate areas, which are research career and personal reasons. First, that we need to research as a PhD student or rather I would say as a graduate One of the core reasons why I took this class is because I do believe it's going to aid in my understanding while I Read through research papers because I realized over the past year that sometimes I kind of struggle in understanding certain concepts whenever I have to go back and watch YouTube videos or try to see if I can get hands-on experience and sometimes that slows me down or that just puts me in a rut. To be able to, you know, get ahead and, you know, be a lot more confident when I recruit papers that have to do with JNI, JNI research. It's also going to help me in my understanding of certain algorithms, tools and techniques. Career-wise, I'm also, due to the advancement in AI right now. My main goal is to, you know, improve my knowledge of skills I gained, my knowledge and skills I gained through Gen AI as that's going to help me with my career prospects which range from having roles from data analyst roles to data science, machine learning and AI engineering roles. While I do have. Especially across from data science, machine learning, and AI engineering roles. On a personal level, I aim to continue deepening my understanding of various concepts that we'll be learning in this class, such as VAEs, GAN models, and LAMs, and also reinforcement learning as well. So before I talk about my expectations I also have a certain degree of knowledge in machine learning concepts and deep learning because I have taken classes such as NLP, I've taken deep learning classes and I've taken machine learning classes over the past year since the start of my program and I've also used the in my same program at the start, I will be able to catch it quickly. So, to my expectations, I would rather keep takeaways from this course. I aim to better understand, you know, our multimodal pipelines, especially like building and training from scratch and for all the downstream tax and analysis. I hope to also deepen my knowledge and understanding of reference-based learning for JNI as well. Know, deepen my knowledge about and have hands-on skills being done. I also plan to develop my skills through hands-on implementation of various models, like I said earlier, such as GANs, VEEs, which I'm not too familiar with currently. I would also especially want to learn, which is one of my core motivations for this course, learn about LLMs and agent coding, because I hope to, in my spare time, be able to build, use this tool to be able to build, develop certain LLMs and agents, LLM agents on my own, and be able to put that in my resume to use, you know, to apply to jobs as well. I also plan to gain knowledge from the upcoming projects I will be embarking on, especially depending on domain I aim to choose, which I'm not too clear on yet, but I do believe within the next couple of weeks I will get an idea of what it is I want to do. Thank you very much.

2:01:21 - Conference Room (Hong Qin) - Speaker 1
I actually really lost track with who is the next student. Sorry, I apologize. Who's the video I just played? Who is the next one?

2:01:42 - Conference Room (Hong Qin) - Speaker 9
is Gaffy.

2:01:44 - Conference Room (Hong Qin) - Speaker 9
Gaffy, you just played Gaffy's video.

2:01:49 - Conference Room (Hong Qin) - Speaker 1
Thank you. Thank you. I'm probably going to miss something. So anyhow. Hello. Okay, let me share the video.

2:02:05 - Unidentified Speaker
My name is Anton Rasmussen.

2:02:09 - Conference Room (Hong Qin) - Speaker 5
And this is my introduce yourself slideshow. So let's see here. So basically, again, my name is Anton Rasmussen. I am a soon to be a master's graduate this fall in computer science at ODU. Obviously this is for CS 795, generative artificial intelligence. I am from Virginia Beach, Virginia, but currently I live in Norfolk, Virginia. One fun fact is I recently started playing bongos. I heard about how Richard Feynman used to play bongos on breaks, and I thought that was a really cool thing to do. So I'm a drummer. I went to Berklee for drum set performance and jazz drumming. So I'm not coming to bongos completely without any percussion background, but it's a fun thing to learn and definitely different from playing the drum set. So some relevant background. I work as a data engineer. I have a little bit of an AI background just academically, but both in my career and in my academics, I have experience with Python, building ML pipelines, and then working with healthcare data. I am familiar with model training, explainability, and de-identification. And then I have previous coursework in ML, data mining, and visualization. I have a lot more coursework than that, but those are maybe the most relevant. Why am I taking this course? Basically, I just want to build more experience with Gen AI, not just the practical experience of using it, which I think we all are doing at this point, but the more academic side the reading the papers, the foundational papers, and then really learning the essential aspects of it. And then this is all in effort to help prepare for a transition to a PhD where I hope to research on healthcare and AI. So I do have a curiosity about ethical and trustworthy AI. I'm actually taking the trustworthy health informatics class as well this semester. So both of those, areas, gen AI and trustworthy AI are right in my kind of in my wheelhouse. Some topics and skills I want to learn. I won't Read them all here, but obviously like the big the big things that I'm interested in are really understanding like transformers, how these things can be tuned, the different types of models that are available and like this, the new terms and acronyms that you see, like RLHF and world models, I'd like to learn a lot more about that. And then again, going back to the trustworthy AI, looking at safety, fairness, alignment, all of that is important to me. I would like also to do some hands-on building and coding of generative models. So I like the idea of this class being project-based and obviously with that comes the ability to perhaps create some new research and potentially a paper. So looking ahead, I would like to, like I said, apply Gen AI to healthcare and then explore more of the agentic AI. We're doing that in my current company quite a bit. So I'd like to lean into that a lot and see what research there is there and hopefully contribute to that research. I'm interested in collaborating with peers. So if anybody is kind of at that healthcare AI, trustworthy AI intersection, please feel free to reach out. And then hopefully, like I said, I can use this course to help gain skills for more academic research and industry roles going forward. So thank you very much and hope you have a great day.

2:06:21 - Conference Room (Hong Qin) - Speaker 1
Take care, bye. Okay. I found the next one. I really appreciate some of you prepared a very nice video. For those that work on a PDF, PowerPoint, I also appreciate you spent effort prepared, but that's not the video. You need to convert that into video. So it's not to say you didn't spend effort, you just say it. If I buy a sandwich, you say, Here's the bread. If you put them together, it's a sandwich. That's not a sandwich. You need to give me the sandwich, not give me the components. If you do this, do that, you will get the sandwich. The point is, if the customer asks you to build a website, you say, well, here's the code. If you run this, you have the website. That's not right. The customer asks you to build a website, you give them a website.

2:07:38 - Conference Room (Hong Qin) - Speaker 8
Good afternoon. My name is Kris Siegfried and today I'll be introducing myself to you professor in the class. So my upbringing, I grew up on a farm outside of Berryville, Virginia. It used to be mainly sheep but switched to cattle about 10 years ago. There I learned how to work with my hands, taking care of animals, building pens, chopping firewood, doing all that great stuff. I actually then went to Woodgrove High School in Percival, Virginia. I was supposed to go to a different high school but I transferred there to pursue a better football program and ultimately academics as a result. I then got offered to go to the University of Richmond and play there. I played my first two years and I stopped to pursue my academics. I ended up graduating with my Bachelor of Arts in Economics and minors in and sociology. Currently, I started my master's of science and data science at ODU in the fall of 2024. I filed a one-year patent for a theory of an AI model in the spring of 2025, and I would love to talk to you about that, professor. I've already talked to a very I guess smarter, I don't know how you want to say it, industry lead. And he said, and that's why I actually filed a patent because he thought it was a really good idea. So I would love your feedback and maybe you could help me with it. Currently, or this past summer, I was a sales operations intern at Zim Shipping, which I did very well using my data science. Continuing throughout the, until I graduate in the spring, I'll be the process excellence intern at SimShipping. So why this class? Well, I love learning. I love learning new things. I actually often Read about AI on my off time and just how the world's changing. I'm very interested in generative AI, not only for my personal interest, but also it's something that could be used at the company I'm currently working at. They've been trying to do stuff with it. So just bringing a little extra knowledge, but again, I am very interested personally too. As I said, it can be applied to my work. And then also I just want to enhance my AI capabilities, keep learning and continue to improve the best I can be and learn the most I can while I'm in this program. And then a little bit about me. I have a dog. I train him for bird hunting. I love to fish. I spend time with family and play games. I just made an uncle this past Friday, which is exciting. And if I won the lottery, I would buy a farm and make my own farm-to-table restaurant. I have a couple of food Allergies, where I've learned to cook. All my siblings like to cook and we all, well, me and my brother like to farm. So it seems like it'd just be a dream to have my family together all working on a farm table restaurant together. I think that'd be really cool.

2:11:10 - Conference Room (Hong Qin) - Speaker 1
So thank you very much.

2:11:13 - Unidentified Speaker
of this course.

2:11:16 - Unidentified Speaker
Thank you. Okay, I found the next video. All right. Hi, everyone.

2:11:26 - Unidentified Speaker
My name is Terry Stilwell, and this is my presentation for CS 895.

2:11:36 - Conference Room (Hong Qin) - Speaker 2
for introductions. So my full name is Terry Stilwell and I'm a second year PhD student in the computer science department. Where am I from So I'm from here in Hampton Read. I was born in Chesapeake and lived in Virginia Beach but I've kind of moved around a lot but I've always been in the So to touch on the relevant background for work experience, I work in the research, sorry, I work in central IT for ODU. We used to be called ITS, but we rebranded to DTT. So if you've ever used the clusters or any cloud resources, you've probably talked to the team I work on. Relevant certifications, so I have a AWS certified AI practitioner cert. That's a mouthful. Really that certificate focuses in on machine learning and generative AI services in AWS. How do you build a model? How do you evaluate it? How do you deploy it in the traditional machine learning sense, but also for generative AI applications, connecting a lot of data sources together, creating knowledge bases for RAG, for chatbots, using the models they have available in their model garden. I'm not sure what they call it. Their listing of models that are available. And how do you tie all of that together and make that available as a service too? We are currently moving a lot of our research focus into Google Cloud. And so right now, I'm trying to learn more about GCP and kind of map the same ideas. Like, how do we do those same things in Google Cloud with Vertex AI? So yeah. So I really work a lot in supporting teaching and research, specifically in the AI ML DL space. And so a lot of that comes with provisioning specific hardware resources. In our data center, but also in the cloud software that can use that, and then building up environments and frameworks that can leverage all of that. So really from a, it's really a support position, making all of that work for people. These are some of the projects that I've helped work on, but the practical AI use is really helping out with daily coding, troubleshooting, and building out some automation. There we go. So to continue that for research experience, my research has focused so far on bioinformatics and high performance computing. Using biological data, you know, you can use, I've had experience with just basic traditional ML methods. Expanding on that a little bit, looking at CNNs for image analysis, and then looking at why the model made that prediction or decision. So just scratch, I've only scratched the surface with model explainability, but that is something I'd like to learn more about, especially with more newer models as well. And then the last two points, looking at multiple types of biological data, combining them, and then using newer tools to hopefully learn more information from that. One example here is single-cell GPT, essentially generating embeddings from one of these pre-trained models and then using those embeddings for analysis further on, and then comparing to a traditional method like XGBoost and seeing why. Is there even an advantage of SCGPT in some of these cases? So exploring some of that.

2:15:58 - Conference Room (Hong Qin) - Speaker 1
So why am I taking this course?

2:16:02 - Conference Room (Hong Qin) - Speaker 2
I really want to deepen my understanding of how Gen AI models work. There's a lot to learn. There's a lot that I don't know, specifically Yeah, going through the syllabus and learning to choose the right model for the right task. And ultimately, for my research, it'd be great to transition from simply using AI tools, which anyone can do, to building with them. Maybe that's building a new model. Maybe that's combining them to build a larger application. But that's ultimately what I think the math and theory behind the models is also my biggest weakness, so just getting more familiar with that. And then with the project and the class, I'm hoping to gain more hands-on practical experience. And so the topics and skills that I'd like to explore, I think agentic AI is really interesting. I think finding the use cases for where that fits, not only my day job, but also of research needs to be figured out. I need to think about that more. But also giving, let's say, LLMs some extra data and some tooling, like MCP, being able to connect to other APIs, actually make it take action. I think that would be fun to learn. Robotics, I think that's interesting, too. We have these Jetson Nano devices from NVIDIA. You can put a webcam on these tiny boards and they can identify objects. That's like the simplest use case, but you can also build cages or attach these to a remote control car and then have that interact in your environment. So it might be cool thinking of a project for that. And then, again, like I mentioned, what's more and what are the latest trends to explore for model explainability? All right. So for fun facts, I am a disinterested Cowboys fan. Not too happy with the team lately, but they play tomorrow, so I'll be watching. The picture there that you see, that's my dog named Griff, and he's a Samoyed. Super fluffy and loses a ton of hair. When I'm not sitting behind a computer screen, you'll find me outside grilling and smoking barbecue. And let's see, I've been at ODU since 2009. So I've been here for a long, long time. And I just joined a bowling league. And so last fun fact, I just bought a new bowling ball and it's actually scented and it smells like Jerry's So that's all I've got. Thanks for listening and I'll see you in class.

2:19:08 - Conference Room (Hong Qin) - Speaker 1
Thank you. Okay, so there's still some student video I cannot play or either in some format I cannot access or in some cases is behind a firewall access control. Apparently, some student actually claim, well, I can see it, everything works on my end. Yeah. If you use that attitude to work with your client, it doesn't, it's not going to work very well. Right. So everything works on your end doesn't mean everything will work on the client's end. So you have to really test it on the client side, from the client perspective. So if you simply say, everything works for me, before I submit it. You seem to even miss the basic software engineering practice. You need to write for the client, not for yourself, right? So the entire mindset is not right, so yeah. Okay, so I'm going to open up... This is interesting. We all have some AI bots. I'm going to kick the AI bots off because otherwise, It's really hard to