## ðŸ’¬ Discussion Questions

<br>

0. What is cross-attention; what are the encoder and decoder?
1. Is recurrence truly obsolete?
2. Can positional encoding be improved for very long windows?
3. How do we reduce compute/memory without hurting accuracy?
4. What are risks of transformer over-reliance?

<!-- I'd love to hear your thoughts on these open questionsâ€”especially whether recurrence might still have a place, or how we might handle compute challenges for extremely long contexts. -->

---